# =====================================================
# Embedding Models Configuration Examples
# =====================================================
# Copy the relevant section to your .env file and uncomment

# =====================================================
# OPTION 1: Jina Embeddings v3 (Cloud)
# =====================================================
# Best for: High quality, cloud deployment, affordable
# MTEB: 66.3 (+6% vs baseline)
# Cost: $0.02 per 1M tokens
# Setup: Get API key from https://jina.ai/embeddings

# JINA_API_KEY=jsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# EMBEDDING_MODEL=jina-embeddings-v3
# LLM_PROVIDER=jina
# EMBEDDING_DIMENSIONS=1024

# =====================================================
# OPTION 2: GTE-Qwen2-7B-instruct (Ollama, Local)
# =====================================================
# Best for: Highest quality, FREE, local deployment
# MTEB: 67.3 (+8% vs baseline)
# Cost: FREE (runs locally via Ollama)
# Setup: ollama pull gte-qwen2-7b-instruct

# EMBEDDING_MODEL=gte-qwen2-7b-instruct
# LLM_PROVIDER=ollama
# EMBEDDING_DIMENSIONS=3584
# LLM_BASE_URL=http://host.docker.internal:11434

# =====================================================
# OPTION 3: OpenAI text-embedding-3-small (Baseline)
# =====================================================
# Best for: Compatibility, established baseline
# MTEB: 62.3 (baseline)
# Cost: $0.02 per 1M tokens

# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# EMBEDDING_MODEL=text-embedding-3-small
# LLM_PROVIDER=openai
# EMBEDDING_DIMENSIONS=1536

# =====================================================
# OPTION 4: OpenAI text-embedding-3-large
# =====================================================
# Best for: Higher quality OpenAI option
# MTEB: 64.6 (+4% vs baseline)
# Cost: $0.13 per 1M tokens (6.5x more expensive)

# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# EMBEDDING_MODEL=text-embedding-3-large
# LLM_PROVIDER=openai
# EMBEDDING_DIMENSIONS=3072

# =====================================================
# OPTION 5: Google text-embedding-004
# =====================================================
# Best for: Multilingual support, Google ecosystem
# MTEB: 62.7 (+0.6% vs baseline)
# Cost: Free tier available, then $0.025 per 1M tokens

# GOOGLE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# EMBEDDING_MODEL=text-embedding-004
# LLM_PROVIDER=google
# EMBEDDING_DIMENSIONS=768

# =====================================================
# OPTION 6: Ollama nomic-embed-text (Local, Free)
# =====================================================
# Best for: Local deployment, no API costs, good balance
# MTEB: 62.4 (+0.2% vs baseline)
# Cost: FREE (runs locally via Ollama)
# Setup: ollama pull nomic-embed-text

# EMBEDDING_MODEL=nomic-embed-text
# LLM_PROVIDER=ollama
# EMBEDDING_DIMENSIONS=768
# LLM_BASE_URL=http://host.docker.internal:11434

# =====================================================
# Additional RAG Strategy Settings
# =====================================================
# These settings apply to all providers

# Batch size for embedding generation (adjust based on provider rate limits)
EMBEDDING_BATCH_SIZE=200

# Enable hybrid search (vector + keyword)
USE_HYBRID_SEARCH=true

# Enable reranking for better search results
USE_RERANKING=true

# Enable contextual embeddings (requires MODEL_CHOICE to be set)
USE_CONTEXTUAL_EMBEDDINGS=false
CONTEXTUAL_EMBEDDINGS_MAX_WORKERS=3

# Enable agentic RAG (code example extraction)
USE_AGENTIC_RAG=true

# LLM model for summaries and contextual embeddings
MODEL_CHOICE=gpt-4.1-nano

# =====================================================
# Provider-Specific Batch Sizes
# =====================================================
# Override batch size per provider if needed

# OpenAI (default: 200)
# OPENAI_EMBEDDING_BATCH_SIZE=200

# Google (default: 100, lower rate limits)
# GOOGLE_EMBEDDING_BATCH_SIZE=100

# Jina (default: 100)
# JINA_EMBEDDING_BATCH_SIZE=100

# Ollama (default: 50, local processing)
# OLLAMA_EMBEDDING_BATCH_SIZE=50

# =====================================================
# Notes
# =====================================================
# 1. Only uncomment ONE embedding model configuration at a time
# 2. Restart Archon after changing embedding settings
# 3. Re-index content to apply new embeddings to existing data
# 4. Monitor rate limits if using cloud providers
# 5. Check provider pricing pages for current costs

# =====================================================
# Recommended Configurations by Use Case
# =====================================================

# PRODUCTION (Best Quality):
# - Use gte-qwen2-7b-instruct (Ollama, FREE, MTEB: 67.3)
# - Fallback: jina-embeddings-v3 (Cloud, $0.02/1M, MTEB: 66.3)

# DEVELOPMENT (Good Balance):
# - Use jina-embeddings-v3 (Cloud, $0.02/1M, MTEB: 66.3)
# - Or: nomic-embed-text (Ollama, FREE, MTEB: 62.4)

# TESTING (Baseline):
# - Use text-embedding-3-small (OpenAI, $0.02/1M, MTEB: 62.3)

# COST-SENSITIVE (Free):
# - Use gte-qwen2-7b-instruct or nomic-embed-text (both Ollama, FREE)

# =====================================================
# Migration Example: From OpenAI to Jina
# =====================================================
# 1. Get Jina API key from https://jina.ai/embeddings
# 2. Update these variables:
#    JINA_API_KEY=jsk-xxx
#    EMBEDDING_MODEL=jina-embeddings-v3
#    LLM_PROVIDER=jina
#    EMBEDDING_DIMENSIONS=1024
# 3. Restart: ./stop-archon.sh && ./start-archon.sh
# 4. Re-index content in Dashboard â†’ Sources
# 5. Verify: Check search quality improvement (+6%)

# =====================================================
# Troubleshooting
# =====================================================
# If embeddings fail:
# 1. Check API key is valid: curl http://localhost:8181/api/settings
# 2. Verify model is available: ollama list (for Ollama)
# 3. Check logs: docker logs archon-backend
# 4. Test endpoint: curl [provider_endpoint]/embeddings
# 5. Verify dimension: SELECT embedding_dimension FROM archon_crawled_pages LIMIT 1

# For detailed troubleshooting, see docs/embedding-models-2024.md
